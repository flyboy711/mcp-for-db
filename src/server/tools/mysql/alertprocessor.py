import logging
import re
from datetime import datetime
from typing import Dict, Any, Sequence, List, Optional
from enum import Enum
from dataclasses import dataclass

from server.tools.mysql import ExecuteSQL
from server.utils.logger import get_logger, configure_logger
from server.tools.mysql.base import BaseHandler
from mcp import Tool
from mcp.types import TextContent

logger = get_logger(__name__)
configure_logger(log_level=logging.INFO, log_filename="ddl_alert_processor.log")


class RiskLevel(Enum):
    """DDLæ“ä½œé£é™©çº§åˆ«"""
    LOW = "ä½é£é™©"
    MEDIUM = "ä¸­é£é™©"
    HIGH = "é«˜é£é™©"
    CRITICAL = "æé«˜é£é™©"


class DDLType(Enum):
    """DDLæ“ä½œç±»å‹"""
    CREATE_TABLE = "æ–°å»ºè¡¨"
    ADD_COLUMN = "åŠ å­—æ®µ"
    ADD_INDEX = "åŠ ç´¢å¼•"
    MODIFY_COLUMN_LENGTH = "åŠ é•¿åº¦"
    MODIFY_COMMENT = "ä¿®æ”¹å¤‡æ³¨"
    MODIFY_INDEX = "ç´¢å¼•ä¿®æ”¹"
    DROP_INDEX = "ç´¢å¼•åˆ é™¤"
    MODIFY_COLUMN_TYPE = "ä¿®æ”¹å­—æ®µç±»å‹"
    MODIFY_UNIQUE_INDEX = "å”¯ä¸€ç´¢å¼•å˜æ›´"
    DROP_TABLE = "åˆ é™¤è¡¨"
    DROP_DATABASE = "åˆ é™¤æ•°æ®åº“"
    DROP_COLUMN = "åˆ é™¤åˆ—"
    RENAME_TABLE = "é‡å‘½åè¡¨"
    TRUNCATE_TABLE = "æ¸…ç©ºè¡¨"
    MIGRATE_TABLE = "åº“è¡¨è¿ç§»"
    DATA_CLEANUP = "æ•°æ®æ¸…ç†"
    ARCHIVE = "å½’æ¡£"


@dataclass
class DDLAlert:
    """DDLå‘Šè­¦ä¿¡æ¯"""
    operation: DDLType
    table: str
    database: str
    table_size_gb: Optional[float] = None
    column: Optional[str] = None
    index: Optional[str] = None
    old_type: Optional[str] = None
    new_type: Optional[str] = None
    is_unique: Optional[bool] = None
    is_archive: Optional[bool] = None
    is_data_cleanup: Optional[bool] = None
    is_migration: Optional[bool] = None


class DDLAlertProcessor(BaseHandler):
    """MySQL DDLå‘Šè­¦æ™ºèƒ½å¤„ç†å™¨"""
    name = "mysql_ddl_alert_processor"
    description = (
        "æ™ºèƒ½å¤„ç†MySQL DDLæ“ä½œå‘Šè­¦ï¼Œè¯„ä¼°é£é™©çº§åˆ«å¹¶æä¾›è§£å†³æ–¹æ¡ˆ"
        "(Intelligently process MySQL DDL operation alerts, assess risk levels and provide solutions)"
    )

    # é£é™©è§„åˆ™å®šä¹‰
    RISK_RULES = {
        RiskLevel.LOW: [
            DDLType.CREATE_TABLE,
            (DDLType.ADD_COLUMN, lambda a: a.table_size_gb and a.table_size_gb < 2),
            (DDLType.ADD_INDEX, lambda a: a.table_size_gb and a.table_size_gb < 2),
            (DDLType.MODIFY_COLUMN_LENGTH, lambda a: a.table_size_gb and a.table_size_gb < 2),
            (DDLType.MODIFY_COMMENT, lambda a: a.table_size_gb and a.table_size_gb < 2),
        ],
        RiskLevel.MEDIUM: [
            (DDLType.ADD_COLUMN, lambda a: a.table_size_gb and 2 <= a.table_size_gb < 50),
            (DDLType.ADD_INDEX, lambda a: a.table_size_gb and 2 <= a.table_size_gb < 50),
            (DDLType.MODIFY_COLUMN_LENGTH, lambda a: a.table_size_gb and 2 <= a.table_size_gb < 50),
            (DDLType.MODIFY_COMMENT, lambda a: a.table_size_gb and 2 <= a.table_size_gb < 50),
            DDLType.MODIFY_INDEX,
            DDLType.DROP_INDEX,
        ],
        RiskLevel.HIGH: [
            (DDLType.ADD_COLUMN, lambda a: a.table_size_gb and a.table_size_gb >= 50),
            (DDLType.ADD_INDEX, lambda a: a.table_size_gb and a.table_size_gb >= 50),
            (DDLType.MODIFY_COLUMN_LENGTH, lambda a: a.table_size_gb and a.table_size_gb >= 50),
            (DDLType.MODIFY_COMMENT, lambda a: a.table_size_gb and a.table_size_gb >= 50),
            DDLType.MODIFY_COLUMN_TYPE,
            DDLType.MODIFY_UNIQUE_INDEX,
        ],
        RiskLevel.CRITICAL: [
            DDLType.DROP_TABLE,
            DDLType.DROP_DATABASE,
            DDLType.DROP_COLUMN,
            DDLType.RENAME_TABLE,
            DDLType.TRUNCATE_TABLE,
            (DDLType.MIGRATE_TABLE, lambda a: not a.is_migration),
            (DDLType.DATA_CLEANUP, lambda a: not a.is_data_cleanup),
            (DDLType.ARCHIVE, lambda a: not a.is_archive),
        ]
    }

    def get_tool_description(self) -> Tool:
        return Tool(
            name=self.name,
            description=self.description,
            inputSchema={
                "type": "object",
                "properties": {
                    "alerts": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "operation": {"type": "string", "description": "DDLæ“ä½œç±»å‹"},
                                "table": {"type": "string", "description": "è¡¨å"},
                                "database": {"type": "string", "description": "æ•°æ®åº“å"},
                                "table_size_gb": {"type": "number", "description": "è¡¨å¤§å°(GB)"},
                                "column": {"type": "string", "description": "åˆ—å(å¯é€‰)"},
                                "index": {"type": "string", "description": "ç´¢å¼•å(å¯é€‰)"},
                                "old_type": {"type": "string", "description": "åŸå­—æ®µç±»å‹(å¯é€‰)"},
                                "new_type": {"type": "string", "description": "æ–°å­—æ®µç±»å‹(å¯é€‰)"},
                                "is_unique": {"type": "boolean", "description": "æ˜¯å¦å”¯ä¸€ç´¢å¼•(å¯é€‰)"},
                                "is_archive": {"type": "boolean", "description": "æ˜¯å¦å½’æ¡£æ“ä½œ(å¯é€‰)"},
                                "is_data_cleanup": {"type": "boolean", "description": "æ˜¯å¦æ•°æ®æ¸…ç†(å¯é€‰)"},
                                "is_migration": {"type": "boolean", "description": "æ˜¯å¦è¿ç§»æ“ä½œ(å¯é€‰)"},
                            },
                            "required": ["operation", "table", "database"]
                        },
                        "description": "å‘Šè­¦ä¿¡æ¯åˆ—è¡¨"
                    }
                },
                "required": ["alerts"]
            }
        )

    async def run_tool(self, arguments: Dict[str, Any]) -> Sequence[TextContent]:
        """å¤„ç†DDLå‘Šè­¦ä¿¡æ¯"""
        try:
            alerts_data = arguments.get("alerts", [])
            if not alerts_data:
                return [TextContent(type="text", text="é”™è¯¯: æœªæä¾›å‘Šè­¦ä¿¡æ¯")]

            # è§£æå‘Šè­¦ä¿¡æ¯
            alerts = [self.parse_alert(alert) for alert in alerts_data]

            # åˆ†æé£é™©çº§åˆ«
            analyzed_alerts = [self.analyze_risk(alert) for alert in alerts]

            # è·å–æ•´ä½“é£é™©çº§åˆ«ï¼ˆå–æœ€é«˜é£é™©ï¼‰
            overall_risk = self.get_overall_risk(analyzed_alerts)

            # ç”Ÿæˆå¤„ç†å»ºè®®
            recommendations = self.generate_recommendations(analyzed_alerts)

            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report(analyzed_alerts, overall_risk, recommendations)

            return [TextContent(type="text", text=report)]

        except Exception as e:
            logger.error(f"å¤„ç†å‘Šè­¦å¤±è´¥: {str(e)}", exc_info=True)
            return [TextContent(type="text", text=f"å¤„ç†å‘Šè­¦å¤±è´¥: {str(e)}")]

    def parse_alert(self, alert_data: Dict[str, Any]) -> DDLAlert:
        """è§£æå‘Šè­¦ä¿¡æ¯"""
        try:
            # è§£ææ“ä½œç±»å‹
            operation = DDLType[alert_data["operation"].upper()]
        except KeyError:
            raise ValueError(f"æœªçŸ¥çš„æ“ä½œç±»å‹: {alert_data['operation']}")

        return DDLAlert(
            operation=operation,
            table=alert_data["table"],
            database=alert_data["database"],
            table_size_gb=alert_data.get("table_size_gb"),
            column=alert_data.get("column"),
            index=alert_data.get("index"),
            old_type=alert_data.get("old_type"),
            new_type=alert_data.get("new_type"),
            is_unique=alert_data.get("is_unique"),
            is_archive=alert_data.get("is_archive"),
            is_data_cleanup=alert_data.get("is_data_cleanup"),
            is_migration=alert_data.get("is_migration"),
        )

    def analyze_risk(self, alert: DDLAlert) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªå‘Šè­¦çš„é£é™©çº§åˆ«"""
        # æ£€æŸ¥æ‰€æœ‰é£é™©çº§åˆ«
        for risk_level, rules in self.RISK_RULES.items():
            for rule in rules:
                if isinstance(rule, tuple):
                    rule_type, condition = rule
                    if alert.operation == rule_type and condition(alert):
                        return {
                            "alert": alert,
                            "risk_level": risk_level,
                            "reason": self.get_risk_reason(alert, risk_level)
                        }
                else:
                    if alert.operation == rule:
                        return {
                            "alert": alert,
                            "risk_level": risk_level,
                            "reason": self.get_risk_reason(alert, risk_level)
                        }

        # é»˜è®¤ä¸­ç­‰é£é™©
        return {
            "alert": alert,
            "risk_level": RiskLevel.MEDIUM,
            "reason": "æœªçŸ¥æ“ä½œç±»å‹ï¼Œé»˜è®¤ä¸­ç­‰é£é™©"
        }

    def get_risk_reason(self, alert: DDLAlert, risk_level: RiskLevel) -> str:
        """è·å–é£é™©åŸå› æè¿°"""
        size_info = f"({alert.table_size_gb}GB)" if alert.table_size_gb else ""

        reasons = {
            RiskLevel.LOW: f"ä½é£é™©æ“ä½œ: {alert.operation.value}{size_info}",
            RiskLevel.MEDIUM: f"ä¸­é£é™©æ“ä½œ: {alert.operation.value}{size_info}",
            RiskLevel.HIGH: f"é«˜é£é™©æ“ä½œ: {alert.operation.value}{size_info}",
            RiskLevel.CRITICAL: f"æé«˜é£é™©æ“ä½œ: {alert.operation.value}"
        }

        # ç‰¹æ®ŠåŸå› è¯´æ˜
        if alert.operation == DDLType.MODIFY_COLUMN_TYPE:
            return f"ä¿®æ”¹å­—æ®µç±»å‹({alert.old_type}â†’{alert.new_type})å±äºé«˜é£é™©æ“ä½œ"
        elif alert.operation == DDLType.MODIFY_UNIQUE_INDEX:
            return "å”¯ä¸€ç´¢å¼•å˜æ›´å±äºé«˜é£é™©æ“ä½œ"
        elif alert.operation in [DDLType.DROP_TABLE, DDLType.DROP_DATABASE, DDLType.DROP_COLUMN]:
            return "åˆ é™¤æ“ä½œå±äºæé«˜é£é™©æ“ä½œ"
        elif alert.operation in [DDLType.RENAME_TABLE, DDLType.TRUNCATE_TABLE]:
            return "ç»“æ„å˜æ›´æ“ä½œå±äºæé«˜é£é™©æ“ä½œ"

        return reasons.get(risk_level, "æœªçŸ¥é£é™©åŸå› ")

    def get_overall_risk(self, analyzed_alerts: List[Dict[str, Any]]) -> RiskLevel:
        """è·å–æ•´ä½“é£é™©çº§åˆ«ï¼ˆå–æœ€é«˜é£é™©ï¼‰"""
        risk_levels = [alert["risk_level"] for alert in analyzed_alerts]

        if RiskLevel.CRITICAL in risk_levels:
            return RiskLevel.CRITICAL
        if RiskLevel.HIGH in risk_levels:
            return RiskLevel.HIGH
        if RiskLevel.MEDIUM in risk_levels:
            return RiskLevel.MEDIUM
        return RiskLevel.LOW

    def generate_recommendations(self, analyzed_alerts: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆå¤„ç†å»ºè®®"""
        recommendations = []

        for alert_info in analyzed_alerts:
            alert = alert_info["alert"]
            risk_level = alert_info["risk_level"]

            if risk_level == RiskLevel.LOW:
                rec = f"âœ… æ“ä½œå®‰å…¨: {alert.operation.value} å¯åœ¨ä¸šåŠ¡ä½å³°æœŸç›´æ¥æ‰§è¡Œ"
            elif risk_level == RiskLevel.MEDIUM:
                rec = (
                    f"âš ï¸ å»ºè®®: {alert.operation.value} æ“ä½œéœ€åœ¨DBAç›‘ç£ä¸‹æ‰§è¡Œ\n"
                    f"   - æ‰§è¡Œå‰å¤‡ä»½æ•°æ®\n"
                    f"   - ä½¿ç”¨pt-online-schema-changeå·¥å…·åœ¨çº¿ä¿®æ”¹\n"
                    f"   - ç›‘æ§æ‰§è¡Œè¿›åº¦"
                )
            elif risk_level == RiskLevel.HIGH:
                rec = (
                    f"ğŸš¨ é«˜é£é™©æ“ä½œ: {alert.operation.value} éœ€è¦ä¸¥æ ¼å®¡æ‰¹\n"
                    f"   - æäº¤è¯¦ç»†å˜æ›´æ–¹æ¡ˆ\n"
                    f"   - åœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯\n"
                    f"   - ä¸šåŠ¡ä½å³°æœŸæ‰§è¡Œ\n"
                    f"   - ä½¿ç”¨åœ¨çº¿DDLå·¥å…·\n"
                    f"   - å‡†å¤‡å›æ»šæ–¹æ¡ˆ"
                )
            else:  # CRITICAL
                rec = (
                    f"ğŸ”¥ æé«˜é£é™©æ“ä½œ: {alert.operation.value} ç¦æ­¢ç›´æ¥æ‰§è¡Œ\n"
                    f"   - éœ€è¦éƒ¨é—¨è´Ÿè´£äººå®¡æ‰¹\n"
                    f"   - æäº¤è¯¦ç»†å½±å“åˆ†ææŠ¥å‘Š\n"
                    f"   - åœ¨æµ‹è¯•ç¯å¢ƒå……åˆ†éªŒè¯\n"
                    f"   - åˆ¶å®šè¯¦ç»†æ‰§è¡Œå’Œå›æ»šè®¡åˆ’\n"
                    f"   - æ‰§è¡Œæ—¶DBAå…¨ç¨‹ç›‘æ§"
                )

            # ç‰¹æ®Šæ“ä½œå»ºè®®
            if alert.operation == DDLType.MODIFY_COLUMN_TYPE:
                rec += (
                    f"\n   - ç‰¹åˆ«æ³¨æ„: ä¿®æ”¹å­—æ®µç±»å‹å¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±æˆ–æˆªæ–­\n"
                    f"     åŸç±»å‹: {alert.old_type} â†’ æ–°ç±»å‹: {alert.new_type}"
                )
            elif alert.operation in [DDLType.DROP_TABLE, DDLType.DROP_DATABASE, DDLType.DROP_COLUMN]:
                rec += (
                    f"\n   - ç‰¹åˆ«æ³¨æ„: åˆ é™¤æ“ä½œä¸å¯é€†ï¼Œå¿…é¡»ç¡®è®¤å¤‡ä»½æœ‰æ•ˆ\n"
                    f"   - å»ºè®®ä½¿ç”¨è½¯åˆ é™¤æˆ–å½’æ¡£ä»£æ›¿ç‰©ç†åˆ é™¤"
                )

            recommendations.append(rec)

        return recommendations

    def generate_report(
            self,
            analyzed_alerts: List[Dict[str, Any]],
            overall_risk: RiskLevel,
            recommendations: List[str]
    ) -> str:
        """ç”Ÿæˆå‘Šè­¦å¤„ç†æŠ¥å‘Š"""
        report = f"# MySQL DDLæ“ä½œé£é™©åˆ†ææŠ¥å‘Š\n\n"
        report += f"**æ•´ä½“é£é™©çº§åˆ«**: {overall_risk.value}\n\n"

        report += "## å‘Šè­¦è¯¦æƒ…\n"
        for i, alert_info in enumerate(analyzed_alerts, 1):
            alert = alert_info["alert"]
            report += (
                f"### å‘Šè­¦ #{i}\n"
                f"- **æ•°æ®åº“**: {alert.database}\n"
                f"- **è¡¨å**: {alert.table}\n"
                f"- **æ“ä½œç±»å‹**: {alert.operation.value}\n"
                f"- **é£é™©çº§åˆ«**: {alert_info['risk_level'].value}\n"
                f"- **é£é™©åŸå› **: {alert_info['reason']}\n"
            )

            if alert.column:
                report += f"- **æ¶‰åŠåˆ—**: {alert.column}\n"
            if alert.index:
                report += f"- **æ¶‰åŠç´¢å¼•**: {alert.index}\n"
            if alert.table_size_gb:
                report += f"- **è¡¨å¤§å°**: {alert.table_size_gb} GB\n"

            report += "\n"

        report += "## å¤„ç†å»ºè®®\n"
        for i, rec in enumerate(recommendations, 1):
            report += f"### å»ºè®® #{i}\n{rec}\n\n"

        report += "## æ•´ä½“å¤„ç†ç­–ç•¥\n"
        if overall_risk == RiskLevel.LOW:
            report += (
                "æ‰€æœ‰æ“ä½œå‡ä¸ºä½é£é™©ï¼Œå¯åœ¨ä¸šåŠ¡ä½å³°æœŸæ‰¹é‡æ‰§è¡Œã€‚\n"
                "å»ºè®®: ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬æ‰§è¡Œï¼Œè®°å½•æ‰§è¡Œæ—¥å¿—ã€‚"
            )
        elif overall_risk == RiskLevel.MEDIUM:
            report += (
                "å­˜åœ¨ä¸­é£é™©æ“ä½œï¼Œéœ€è¦è°¨æ…å¤„ç†ã€‚\n"
                "å»ºè®®:\n"
                "  - åˆ¶å®šè¯¦ç»†æ‰§è¡Œè®¡åˆ’\n"
                "  - ä¸šåŠ¡ä½å³°æœŸæ‰§è¡Œ\n"
                "  - DBAç›‘ç£æ‰§è¡Œè¿‡ç¨‹\n"
                "  - å‡†å¤‡å›æ»šæ–¹æ¡ˆ"
            )
        elif overall_risk == RiskLevel.HIGH:
            report += (
                "å­˜åœ¨é«˜é£é™©æ“ä½œï¼Œéœ€è¦ä¸¥æ ¼å®¡æ‰¹å’Œå……åˆ†å‡†å¤‡ã€‚\n"
                "å»ºè®®:\n"
                "  - æäº¤å˜æ›´å®¡æ‰¹æµç¨‹\n"
                "  - åœ¨æµ‹è¯•ç¯å¢ƒå……åˆ†éªŒè¯\n"
                "  - ä½¿ç”¨åœ¨çº¿DDLå·¥å…·\n"
                "  - åˆ¶å®šè¯¦ç»†å›æ»šè®¡åˆ’\n"
                "  - æ‰§è¡Œæ—¶DBAå…¨ç¨‹ç›‘æ§"
            )
        else:
            report += (
                "å­˜åœ¨æé«˜é£é™©æ“ä½œï¼Œç¦æ­¢ç›´æ¥æ‰§è¡Œï¼\n"
                "å¿…é¡»:\n"
                "  - æäº¤éƒ¨é—¨è´Ÿè´£äººå®¡æ‰¹\n"
                "  - æä¾›è¯¦ç»†å½±å“åˆ†ææŠ¥å‘Š\n"
                "  - åœ¨æµ‹è¯•ç¯å¢ƒå……åˆ†éªŒè¯\n"
                "  - åˆ¶å®šè¯¦ç»†æ‰§è¡Œå’Œå›æ»šè®¡åˆ’\n"
                "  - æ‰§è¡Œæ—¶DBAå’Œå¼€å‘äººå‘˜å…¨ç¨‹åœ¨åœº\n"
                "  - å»ºè®®è€ƒè™‘æ›¿ä»£æ–¹æ¡ˆï¼Œé¿å…ç›´æ¥æ‰§è¡Œé«˜å±æ“ä½œ"
            )

        report += "\n\n**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: " + datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return report


########################################################################################################################
########################################################################################################################
class AlertDiscoveryTool(BaseHandler):
    """å‘Šè­¦ä¿¡æ¯å‘ç°å·¥å…·"""
    name = "alert_discovery"
    description = (
        "åœ¨æ•°æ®åº“ä¸­è‡ªåŠ¨å‘ç°å‘Šè­¦ä¿¡æ¯è¡¨å’Œå­—æ®µ"
        "(Automatically discover alert-related tables and columns in the database)"
    )

    def get_tool_description(self) -> Tool:
        return Tool(
            name=self.name,
            description=self.description,
            inputSchema={
                "type": "object",
                "properties": {
                    "keywords": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "æœç´¢å…³é”®è¯åˆ—è¡¨ï¼ˆå¦‚['alert', 'alarm', 'warning']ï¼‰",
                        "default": ["alert", "alarm", "warning", "error"]
                    },
                    "search_depth": {
                        "type": "integer",
                        "description": "æœç´¢æ·±åº¦ï¼ˆ1-3ï¼‰ï¼Œ1=è¡¨åï¼Œ2=åˆ—åï¼Œ3=æ³¨é‡Š",
                        "default": 3
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡",
                        "default": 20
                    }
                },
                "required": []
            }
        )

    async def run_tool(self, arguments: Dict[str, Any]) -> Sequence[TextContent]:
        """åœ¨æ•°æ®åº“ä¸­æœç´¢å‘Šè­¦ä¿¡æ¯è¡¨å’Œå­—æ®µ"""
        try:
            keywords = arguments.get("keywords", ["alert", "alarm", "warning", "error"])
            search_depth = min(max(arguments.get("search_depth", 3), 1), 3)
            max_results = arguments.get("max_results", 20)

            logger.info(f"å¼€å§‹å‘Šè­¦ä¿¡æ¯å‘ç°: å…³é”®è¯={keywords}, æ·±åº¦={search_depth}")

            # è·å–æ•°æ®åº“è¿æ¥
            execute_sql = ExecuteSQL()

            # æ„å»ºæœç´¢æ¡ä»¶
            conditions = []
            for keyword in keywords:
                escaped_keyword = keyword.replace("'", "''")

                # è¡¨åæœç´¢
                if search_depth >= 1:
                    conditions.append(f"TABLE_NAME LIKE '%{escaped_keyword}%'")

                # åˆ—åæœç´¢
                if search_depth >= 2:
                    conditions.append(f"COLUMN_NAME LIKE '%{escaped_keyword}%'")

                # æ³¨é‡Šæœç´¢
                if search_depth >= 3:
                    conditions.append(f"TABLE_COMMENT LIKE '%{escaped_keyword}%'")
                    conditions.append(f"COLUMN_COMMENT LIKE '%{escaped_keyword}%'")

            where_clause = " OR ".join(conditions) if conditions else "1=1"

            # æ„å»ºæŸ¥è¯¢è¯­å¥
            query = f"""
                SELECT 
                    TABLE_SCHEMA AS database_name,
                    TABLE_NAME AS table_name,
                    COLUMN_NAME AS column_name,
                    DATA_TYPE AS data_type,
                    COLUMN_COMMENT AS column_comment,
                    TABLE_COMMENT AS table_comment,
                    CASE 
                        WHEN COLUMN_NAME IS NOT NULL THEN 'column'
                        ELSE 'table'
                    END AS match_type
                FROM information_schema.COLUMNS
                WHERE ({where_clause})
                UNION
                SELECT 
                    TABLE_SCHEMA AS database_name,
                    TABLE_NAME AS table_name,
                    NULL AS column_name,
                    NULL AS data_type,
                    NULL AS column_comment,
                    TABLE_COMMENT AS table_comment,
                    'table' AS match_type
                FROM information_schema.TABLES
                WHERE ({where_clause.replace('COLUMN_COMMENT', 'TABLE_COMMENT')})
                ORDER BY database_name, table_name, column_name
                LIMIT {max_results}
            """

            logger.debug(f"æ‰§è¡ŒæŸ¥è¯¢: {query}")

            # æ‰§è¡ŒæŸ¥è¯¢
            results = await execute_sql.run_tool({"query": query})

            # å¤„ç†ç»“æœ
            if not results:
                return [TextContent(type="text", text="æœªæ‰¾åˆ°åŒ¹é…çš„å‘Šè­¦ä¿¡æ¯è¡¨æˆ–å­—æ®µ")]

            # è§£æç»“æœ - å‡è®¾æ‰€æœ‰ç»“æœéƒ½æ˜¯TextContentç±»å‹
            alert_tables = {}
            for result in results:
                if isinstance(result, TextContent):
                    # è§£æCSVæ ¼å¼çš„ç»“æœ
                    lines = result.text.strip().split('\n')
                    if not lines:
                        continue

                    # æå–è¡¨å¤´
                    headers = [h.strip() for h in lines[0].split(',')]

                    # å¤„ç†æ•°æ®è¡Œ
                    for line in lines[1:]:
                        values = [v.strip() for v in line.split(',')]
                        if len(values) >= len(headers):
                            row = dict(zip(headers, values))
                            self._process_row(row, alert_tables)

            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report(alert_tables, keywords)
            return [TextContent(type="text", text=report)]

        except Exception as e:
            logger.error(f"å‘Šè­¦ä¿¡æ¯å‘ç°å¤±è´¥: {str(e)}", exc_info=True)
            return [TextContent(type="text", text=f"å‘Šè­¦ä¿¡æ¯å‘ç°å¤±è´¥: {str(e)}")]

    def _process_row(self, row: Dict[str, str], alert_tables: Dict[str, Dict]):
        """å¤„ç†æŸ¥è¯¢ç»“æœè¡Œ"""
        db_table = f"{row['database_name']}.{row['table_name']}"

        # åˆå§‹åŒ–è¡¨ä¿¡æ¯
        if db_table not in alert_tables:
            alert_tables[db_table] = {
                "database": row['database_name'],
                "table": row['table_name'],
                "comment": row.get('table_comment', ''),
                "columns": []
            }

        # æ·»åŠ åˆ—ä¿¡æ¯
        if row.get('column_name'):
            alert_tables[db_table]["columns"].append({
                "name": row['column_name'],
                "type": row.get('data_type', ''),
                "comment": row.get('column_comment', '')
            })

    def generate_report(self, alert_tables: Dict[str, Dict], keywords: List[str]) -> str:
        """ç”Ÿæˆå‘Šè­¦ä¿¡æ¯å‘ç°æŠ¥å‘Š"""
        report = "# å‘Šè­¦ä¿¡æ¯å‘ç°æŠ¥å‘Š\n\n"
        report += f"**æœç´¢å…³é”®è¯**: {', '.join(keywords)}\n\n"

        if not alert_tables:
            report += "æœªå‘ç°åŒ¹é…çš„å‘Šè­¦ä¿¡æ¯è¡¨æˆ–å­—æ®µ\n"
            return report

        report += "## å‘ç°çš„å‘Šè­¦ä¿¡æ¯è¡¨\n"

        for table_info in alert_tables.values():
            report += f"### æ•°æ®åº“: `{table_info['database']}` è¡¨å: `{table_info['table']}`\n"

            if table_info['comment']:
                report += f"**è¡¨æ³¨é‡Š**: {table_info['comment']}\n"

            if table_info['columns']:
                report += "**ç›¸å…³å­—æ®µ**:\n"
                for col in table_info['columns']:
                    report += f"- `{col['name']}` ({col['type']})"
                    if col['comment']:
                        report += f": {col['comment']}"
                    report += "\n"
            else:
                report += "> è¡¨ååŒ¹é…ä½†æœªå‘ç°ç›¸å…³å­—æ®µ\n"

            report += "\n"

        report += "## åç»­æ­¥éª¤å»ºè®®\n"
        report += ("1. éªŒè¯å‘ç°çš„è¡¨æ˜¯å¦ç¡®å®åŒ…å«å‘Šè­¦ä¿¡æ¯\n"
                   "2. ä½¿ç”¨æ•°æ®æŸ¥è¯¢å·¥å…·è¿›ä¸€æ­¥åˆ†æå‘Šè­¦æ•°æ®\n"
                   "3. åˆ›å»ºå‘Šè­¦åˆ†æè§†å›¾æˆ–ç‰©åŒ–è§†å›¾\n"
                   "4. è®¾ç½®å®šæœŸå‘Šè­¦åˆ†æä»»åŠ¡\n")

        report += "\n> **æ³¨æ„**: æœ¬æŠ¥å‘ŠåŸºäºå…ƒæ•°æ®æœç´¢ç”Ÿæˆï¼Œå®é™…å†…å®¹éœ€è¿›ä¸€æ­¥éªŒè¯"

        return report


########################################################################################################################
########################################################################################################################
class AlertDataAnalyzer(BaseHandler):
    """å‘Šè­¦æ•°æ®åˆ†æå·¥å…·"""
    name = "alert_data_analyzer"
    description = (
        "åˆ†æå‘Šè­¦æ•°æ®å¹¶æä¾›è§£å†³æ–¹æ¡ˆ"
        "(Analyze alert data and provide solutions)"
    )

    def get_tool_description(self) -> Tool:
        return Tool(
            name=self.name,
            description=self.description,
            inputSchema={
                "type": "object",
                "properties": {
                    "database": {
                        "type": "string",
                        "description": "æ•°æ®åº“åç§°"
                    },
                    "table": {
                        "type": "string",
                        "description": "è¡¨å"
                    },
                    "time_range": {
                        "type": "object",
                        "properties": {
                            "start": {"type": "string", "description": "å¼€å§‹æ—¶é—´ï¼ˆYYYY-MM-DD HH:MM:SSï¼‰"},
                            "end": {"type": "string", "description": "ç»“æŸæ—¶é—´ï¼ˆYYYY-MM-DD HH:MM:SSï¼‰"}
                        },
                        "description": "æ—¶é—´èŒƒå›´ï¼ˆå¯é€‰ï¼‰"
                    },
                    "filters": {
                        "type": "object",
                        "description": "é™„åŠ è¿‡æ»¤æ¡ä»¶ï¼ˆå¯é€‰ï¼‰"
                    },
                    "analysis_depth": {
                        "type": "string",
                        "enum": ["summary", "detailed", "root_cause"],
                        "description": "åˆ†ææ·±åº¦",
                        "default": "summary"
                    }
                },
                "required": ["database", "table"]
            }
        )

    async def run_tool(self, arguments: Dict[str, Any]) -> Sequence[TextContent]:
        """åˆ†æå‘Šè­¦æ•°æ®"""
        try:
            database = arguments["database"]
            table = arguments["table"]
            time_range = arguments.get("time_range")
            filters = arguments.get("filters", {})
            analysis_depth = arguments.get("analysis_depth", "summary")

            logger.info(f"å¼€å§‹åˆ†æå‘Šè­¦æ•°æ®: {database}.{table}")

            # è·å–è¡¨ç»“æ„
            table_structure = await self.get_table_structure(database, table)

            # åˆ†æå‘Šè­¦å­—æ®µ
            alert_fields = self.identify_alert_fields(table_structure)

            if not alert_fields:
                return [TextContent(type="text", text=f"æœªèƒ½åœ¨è¡¨ {table} ä¸­è¯†åˆ«å‘Šè­¦å­—æ®µ")]

            # è·å–å‘Šè­¦æ•°æ®
            alert_data = await self.get_alert_data(database, table, alert_fields, time_range, filters)

            # åˆ†æå‘Šè­¦æ•°æ®
            analysis = self.analyze_alerts(alert_data, alert_fields, analysis_depth)

            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report(database, table, alert_fields, analysis)

            return [TextContent(type="text", text=report)]

        except Exception as e:
            logger.error(f"å‘Šè­¦æ•°æ®åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
            return [TextContent(type="text", text=f"å‘Šè­¦æ•°æ®åˆ†æå¤±è´¥: {str(e)}")]

    async def get_table_structure(self, database: str, table: str) -> List[Dict]:
        """è·å–è¡¨ç»“æ„ä¿¡æ¯"""
        execute_sql = ExecuteSQL()

        query = f"""
            SELECT 
                COLUMN_NAME, DATA_TYPE, COLUMN_COMMENT
            FROM information_schema.COLUMNS
            WHERE TABLE_SCHEMA = '{database}' 
              AND TABLE_NAME = '{table}'
        """

        results = await execute_sql.run_tool({"query": query})

        # è§£æç»“æœ - åªå¤„ç†TextContentç±»å‹
        structure = []
        for result in results:
            if isinstance(result, TextContent):
                lines = result.text.strip().split('\n')
                if not lines:
                    continue

                # æå–è¡¨å¤´
                headers = [h.strip() for h in lines[0].split(',')]

                # å¤„ç†æ•°æ®è¡Œ
                for line in lines[1:]:
                    values = [v.strip() for v in line.split(',')]
                    if len(values) >= len(headers):
                        row = dict(zip(headers, values))
                        structure.append(row)

        return structure

    def identify_alert_fields(self, structure: List[Dict]) -> Dict[str, str]:
        """è¯†åˆ«å‘Šè­¦ç›¸å…³å­—æ®µ"""
        alert_fields = {}

        # å¸¸è§å‘Šè­¦å­—æ®µæ¨¡å¼
        patterns = {
            "level": r"level|severity|priority",
            "type": r"type|category",
            "message": r"message|content|description",
            "timestamp": r"time|timestamp|created_at",
            "source": r"source|origin|host",
            "status": r"status|state"
        }

        for col in structure:
            col_name = col.get("COLUMN_NAME", "")
            col_comment = col.get("COLUMN_COMMENT", "")

            for field_type, pattern in patterns.items():
                if (re.search(pattern, col_name, re.IGNORECASE) or
                        re.search(pattern, col_comment, re.IGNORECASE)):
                    alert_fields[field_type] = col_name
                    break

        return alert_fields

    async def get_alert_data(self, database: str, table: str, alert_fields: Dict,
                             time_range: Optional[Dict], filters: Dict) -> List[Dict]:
        """è·å–å‘Šè­¦æ•°æ®"""
        execute_sql = ExecuteSQL()

        # æ„å»ºæŸ¥è¯¢å­—æ®µ
        select_fields = list(alert_fields.values())
        if not select_fields:
            return []

        # æ„å»ºWHEREæ¡ä»¶
        conditions = []

        # æ—¶é—´èŒƒå›´æ¡ä»¶
        if time_range and "timestamp" in alert_fields:
            time_field = alert_fields["timestamp"]
            conditions.append(f"`{time_field}` BETWEEN '{time_range['start']}' AND '{time_range['end']}'")

        # é™„åŠ è¿‡æ»¤æ¡ä»¶
        for field, value in filters.items():
            conditions.append(f"`{field}` = '{value}'")

        where_clause = " AND ".join(conditions) if conditions else "1=1"

        # æ„å»ºæŸ¥è¯¢
        query = f"""
            SELECT {', '.join([f'`{f}`' for f in select_fields])}
            FROM `{database}`.`{table}`
            WHERE {where_clause}
            LIMIT 1000
        """

        results = await execute_sql.run_tool({"query": query})

        # è§£æç»“æœ - åªå¤„ç†TextContentç±»å‹
        data = []
        for result in results:
            if isinstance(result, TextContent):
                lines = result.text.strip().split('\n')
                if not lines:
                    continue

                # æå–è¡¨å¤´
                headers = [h.strip() for h in lines[0].split(',')]

                # å¤„ç†æ•°æ®è¡Œ
                for line in lines[1:]:
                    values = [v.strip() for v in line.split(',')]
                    if len(values) >= len(headers):
                        row = dict(zip(headers, values))
                        data.append(row)

        return data

    def analyze_alerts(self, alert_data: List[Dict], alert_fields: Dict, depth: str) -> Dict:
        """åˆ†æå‘Šè­¦æ•°æ®"""
        analysis = {
            "summary": {},
            "trends": {},
            "patterns": {},
            "recommendations": []
        }

        if not alert_data:
            return analysis

        # åŸºæœ¬ç»Ÿè®¡
        total_alerts = len(alert_data)

        # æŒ‰çº§åˆ«ç»Ÿè®¡
        if "level" in alert_fields:
            level_field = alert_fields["level"]
            level_counts = {}
            for alert in alert_data:
                level = alert.get(level_field, "unknown")
                level_counts[level] = level_counts.get(level, 0) + 1

            analysis["summary"]["level_distribution"] = level_counts

        # æŒ‰ç±»å‹ç»Ÿè®¡
        if "type" in alert_fields:
            type_field = alert_fields["type"]
            type_counts = {}
            for alert in alert_data:
                alert_type = alert.get(type_field, "unknown")
                type_counts[alert_type] = type_counts.get(alert_type, 0) + 1

            analysis["summary"]["type_distribution"] = type_counts
            analysis["recommendations"].append("æ£€æŸ¥é«˜é¢‘å‘Šè­¦ç±»å‹ï¼Œä¼˜åŒ–ç›¸å…³ç³»ç»Ÿ")

        # æ—¶é—´è¶‹åŠ¿åˆ†æ
        if "timestamp" in alert_fields and depth in ["detailed", "root_cause"]:
            time_field = alert_fields["timestamp"]
            time_series = {}
            for alert in alert_data:
                # ç®€åŒ–æ—¶é—´åˆ°å°æ—¶çº§åˆ«
                time_key = alert[time_field][:13] + ":00:00"
                time_series[time_key] = time_series.get(time_key, 0) + 1

            analysis["trends"]["hourly_distribution"] = time_series

            # æ£€æµ‹é«˜å³°æ—¶æ®µ
            max_count = max(time_series.values())
            peak_hours = [hour for hour, count in time_series.items() if count == max_count]
            analysis["patterns"]["peak_hours"] = peak_hours
            analysis["recommendations"].append(f"é«˜å³°æ—¶æ®µ: {', '.join(peak_hours)}ï¼Œå»ºè®®åŠ å¼ºç›‘æ§")

        # æ ¹å› åˆ†æï¼ˆæ·±åº¦æ¨¡å¼ï¼‰
        if depth == "root_cause" and "source" in alert_fields and "message" in alert_fields:
            source_field = alert_fields["source"]
            message_field = alert_fields["message"]

            # åˆ†æå¸¸è§é”™è¯¯æ¶ˆæ¯
            error_patterns = {}
            for alert in alert_data:
                message = alert.get(message_field, "")
                if "error" in message.lower() or "fail" in message.lower():
                    # æå–é”™è¯¯å…³é”®è¯
                    match = re.search(r'\b(error|failed|exception|timeout)\b', message, re.IGNORECASE)
                    if match:
                        error_key = match.group(0).lower()
                        error_patterns[error_key] = error_patterns.get(error_key, 0) + 1

            analysis["patterns"]["error_patterns"] = error_patterns

            # åˆ†ææ¥æºç³»ç»Ÿ
            source_counts = {}
            for alert in alert_data:
                source = alert.get(source_field, "unknown")
                source_counts[source] = source_counts.get(source, 0) + 1

            analysis["patterns"]["source_distribution"] = source_counts

            # ç”Ÿæˆæ ¹å› å»ºè®®
            if error_patterns:
                top_error = max(error_patterns, key=error_patterns.get)
                analysis["recommendations"].append(f"ä¸»è¦é”™è¯¯ç±»å‹: {top_error}ï¼Œå»ºè®®æ£€æŸ¥ç›¸å…³ç³»ç»Ÿæ—¥å¿—")

            if source_counts:
                top_source = max(source_counts, key=source_counts.get)
                analysis["recommendations"].append(f"ä¸»è¦å‘Šè­¦æ¥æº: {top_source}ï¼Œå»ºè®®é‡ç‚¹æ’æŸ¥")

        # æ·»åŠ é€šç”¨å»ºè®®
        analysis["recommendations"].extend([
            "å®šæœŸå®¡æŸ¥å‘Šè­¦è§„åˆ™ï¼Œå‡å°‘è¯¯æŠ¥",
            "è®¾ç½®å‘Šè­¦åˆ†çº§å“åº”æœºåˆ¶",
            "å»ºç«‹å‘Šè­¦é—­ç¯å¤„ç†æµç¨‹"
        ])

        return analysis

    def generate_report(self, database: str, table: str, alert_fields: Dict, analysis: Dict) -> str:
        """ç”Ÿæˆå‘Šè­¦åˆ†ææŠ¥å‘Š"""
        report = f"# å‘Šè­¦æ•°æ®åˆ†ææŠ¥å‘Š\n\n"
        report += f"**æ•°æ®åº“**: `{database}`\n"
        report += f"**è¡¨å**: `{table}`\n\n"

        report += "## å‘Šè­¦å­—æ®µè¯†åˆ«\n"
        for field_type, field_name in alert_fields.items():
            report += f"- **{field_type}**: `{field_name}`\n"
        report += "\n"

        report += "## åˆ†ææ‘˜è¦\n"
        if "summary" in analysis:
            for section, data in analysis["summary"].items():
                report += f"### {section.replace('_', ' ').title()}\n"
                if isinstance(data, dict):
                    for key, value in data.items():
                        report += f"- {key}: {value}\n"
                else:
                    report += f"{data}\n"
                report += "\n"

        if "trends" in analysis and analysis["trends"]:
            report += "## æ—¶é—´è¶‹åŠ¿åˆ†æ\n"
            for section, data in analysis["trends"].items():
                report += f"### {section.replace('_', ' ').title()}\n"
                if isinstance(data, dict):
                    report += "| æ—¶é—´ | å‘Šè­¦æ•°é‡ |\n"
                    report += "|------|----------|\n"
                    for time, count in data.items():
                        report += f"| {time} | {count} |\n"
                report += "\n"

        if "patterns" in analysis and analysis["patterns"]:
            report += "## æ¨¡å¼è¯†åˆ«\n"
            for section, data in analysis["patterns"].items():
                report += f"### {section.replace('_', ' ').title()}\n"
                if isinstance(data, dict):
                    for key, value in data.items():
                        report += f"- {key}: {value}\n"
                elif isinstance(data, list):
                    report += ", ".join(data)
                report += "\n"

        if "recommendations" in analysis and analysis["recommendations"]:
            report += "## ä¼˜åŒ–å»ºè®®\n"
            for i, rec in enumerate(analysis["recommendations"], 1):
                report += f"{i}. {rec}\n"

        report += "\n## åç»­æ­¥éª¤\n"
        report += ("1. éªŒè¯åˆ†æç»“æœå‡†ç¡®æ€§\n"
                   "2. å®æ–½ä¼˜åŒ–å»ºè®®\n"
                   "3. ç›‘æ§å‘Šè­¦å˜åŒ–è¶‹åŠ¿\n"
                   "4. å®šæœŸé‡æ–°åˆ†æå‘Šè­¦æ•°æ®\n")

        return report

########################################################################################################################
########################################################################################################################
